{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science: Choose Your Own Adventure\n",
    "**By Mac-I and Sophia**\n",
    "\n",
    "For this project, we decided to use the [San Franscisco Crime Dataset](https://www.kaggle.com/c/sf-crime), with the goal of predicting the category of crime based on the date/time of report, and the location of the report. This notebook will serve as a writeup of the work that we have done on this project, in both data exploration and building a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Before we get started, let's talk about what we're trying to do and what information we actually have!\n",
    "\n",
    "In the dataset, the information we have is:\n",
    "* **Dates**\n",
    "* Category\n",
    "* **Description**\n",
    "* **Day of Week**\n",
    "* **Police District**\n",
    "* Resolution\n",
    "* **Addresss**\n",
    "* **X (Longitude)**\n",
    "* **Y (Latitude)**\n",
    "\n",
    "The bolded items are the ones that occur in both the test and training datasets. In other words, the bolded items are the ones that we will be using to predict the cateogry of the crime. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Everything\n",
    "To keep our code neat, let's import all the helper libraries we need up here!\n",
    "\n",
    "We will also read in the data here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import shapefile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "#data exploration imports\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib import cm\n",
    "from datetime import datetime\n",
    "from ipywidgets import widgets  \n",
    "from IPython.display import display\n",
    "\n",
    "#Building and testing model iputs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "\n",
    "isPowerful = False\n",
    "\n",
    "crimeData = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Before we get into building a model, we're going to start by just exploring the dataset. The goal of this is to just explore what kinds of relationships exist in the dataset. \n",
    "\n",
    "In the dataset, we have two different kinds of data: location data and time data that we can use to predict the type of crime. \n",
    "\n",
    "First, we'll start by exploring how location plays a role in the type of crime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Relationship between Location and Crime Category\n",
    "In the graphs below, we'll show some of the work we did to explore how location and the category of crime are related. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Model\n",
    "Now that we've explored the data, we're going to start building a model. To make our lives easier, we're going to follow a pretty simple workflow. We will \n",
    "1. Read in the Data\n",
    "2. Clean the training data\n",
    "3. Create a Model using the cleaned data\n",
    "4. Score the model using the crime categories in the training data\n",
    "5. If the model performs well (better than previous attempts) we will: \n",
    "  1. Repeat steps 1-3 with the test data. \n",
    "  2. Generate a submission file to upload to Kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data and constants\n",
    "Here we load the data into a panda dataframe and set some initial constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crimeData = pd.read_csv('train.csv')\n",
    "Categories = ['ARSON', 'ASSAULT', 'BAD CHECKS', 'BRIBERY', 'BURGLARY',\n",
    "              'DISORDERLY CONDUCT', 'DRIVING UNDER THE INFLUENCE',\n",
    "              'DRUG/NARCOTIC', 'DRUNKENNESS', 'EMBEZZLEMENT', 'EXTORTION',\n",
    "              'FAMILY OFFENSES', 'FORGERY/COUNTERFEITING', 'FRAUD', 'GAMBLING',\n",
    "              'KIDNAPPING', 'LARCENY/THEFT', 'LIQUOR LAWS', 'LOITERING',\n",
    "              'MISSING PERSON', 'NON-CRIMINAL', 'OTHER OFFENSES',\n",
    "              'PORNOGRAPHY/OBSCENE MAT', 'PROSTITUTION', 'RECOVERED VEHICLE',\n",
    "              'ROBBERY', 'RUNAWAY', 'SECONDARY CODES', 'SEX OFFENSES FORCIBLE',\n",
    "              'SEX OFFENSES NON FORCIBLE', 'STOLEN PROPERTY', 'SUICIDE',\n",
    "              'SUSPICIOUS OCC', 'TREA', 'TRESPASS', 'VANDALISM', 'VEHICLE THEFT',\n",
    "              'WARRANTS', 'WEAPON LAWS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "This section will cover how we cleaned the data as well as how we added features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodeData(df, isTrain = False):\n",
    "    '''This function takes in the dataframe that we get from loading in the \n",
    "    SF crime data and returns a re-coded dataframe that has all the \n",
    "    additional features we want to add and the categorical features recoded \n",
    "    and cleaned.\n",
    "    '''\n",
    "\n",
    "    #since the modifications are done in-place we don't return the dataframe. \n",
    "    #we do, however, return the list of all the columns we added.\n",
    "    df, newLatLon = removeOutlierLatLon(df)\n",
    "    df, newDate = recodeDates(df)\n",
    "    df, newDistrict = recodePoliceDistricts(df)\n",
    "    print \"recoding addresses\"\n",
    "    df, newAddress, streetColumns = recodeAddresses(df)\n",
    "    df, newWeather = addWeather(df)\n",
    "\n",
    "    \n",
    "    addedColumns = [] \n",
    "    addedColumns += newDate\n",
    "    addedColumns += newDistrict \n",
    "    addedColumns += newLatLon\n",
    "    addedColumns += newAddress\n",
    "    addedColumns += newWeather\n",
    "   \n",
    "\n",
    "    if (isTrain):\n",
    "        newCategory = recodeCategories(df)\n",
    "        addedColumns += newCategory\n",
    "        try: #prevents error if the columns have already been removed or we are processing test data\n",
    "            columnsToDrop = ['Descript', 'Resolution']\n",
    "            df.drop(columnsToDrop, axis=1, inplace=True)\n",
    "        except:\n",
    "            print \"already recoded or using test data\"\n",
    "         \n",
    "\n",
    "    return df, addedColumns, streetColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recode the Categories\n",
    "Here we turn the category names into integers to ease classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodeCategories(df):\n",
    "    '''This function will recode the Categories from strings into integers'''\n",
    "    #if 'Category' in df.columns:\n",
    "    df['CategoryRecode'] = df.Category.apply(lambda x: Categories.index(x))\n",
    "        \n",
    "    return df, ['CategoryRecode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing the Latitudes and Longitudes that do fall in San Francisco\n",
    "During our data exploration we noticed that some of the latitudes and longitudes listed were not anywhere close to San Francisco. In order to fix this we calculated the median latidute and logitude for each police district. We then assigned the appropriate median latitude and logitude to those data points with invalid latitude and logitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeOutlierLatLon(df):\n",
    "    '''This function will remove outlier Latitudes and Longitudes'''\n",
    "    df.loc[df.X > -121, 'X'] = df.loc[(df.X > -121)].apply(lambda row: df.X[df[\"PdDistrict\"] == row['PdDistrict']].median(), axis=1)\n",
    "    df.loc[df.Y > 38, 'Y'] = df.loc[(df.Y > 38)].apply(lambda row: df.Y[df[\"PdDistrict\"] == row['PdDistrict']].median(), axis=1)\n",
    "\n",
    "    return df, ['X', 'Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recoding Dates\n",
    "In order to make the \"Dates\" column useful we needed to recode them into columns such as \"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\". We also needed to recode the \"DayOfWeek\" into a nurerical format since some of the models can't handle string categorical data. It also makes sense from the perspective that a model may want to group data by weekday vs weekend which is much easier with numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodeDates(df):\n",
    "    '''This function takes in a dataframe and recodes the date field into \n",
    "    useable values. Here, we also recode the day of week.'''\n",
    "    #Recode the dates column to year, month, day and hour columns\n",
    "    df['DateTime'] = df['Dates'].apply(\n",
    "        lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    df['Year'] = df['DateTime'].apply(lambda x: x.year)\n",
    "    df['Month'] = df['DateTime'].apply(lambda x: x.month)\n",
    "    df['Day'] = df['DateTime'].apply(lambda x: x.day)\n",
    "    df['Hour'] = df['DateTime'].apply(lambda x: x.hour)\n",
    "    df['Minute'] = df['DateTime'].apply(lambda x: x.minute)\n",
    "    df['DayOfWeekRecode'] = df['DateTime'].apply(lambda x: x.weekday())\n",
    "\n",
    "    return df, ['Year', 'Month', 'Day', 'Hour', 'Minute', 'DayOfWeekRecode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recoding the Police districts\n",
    "Similarly to the \"DayOfWeek\" column, the \"PdDistrict\" column needed to be recoded in order to be useful. We did this with one-hot encoding since the there is no inherent order to the districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodePoliceDistricts(df):\n",
    "    '''This function recodes the police district to a one-hot encoding scheme.'''\n",
    "    districts = df['PdDistrict'].unique().tolist()\n",
    "    newColumns = []\n",
    "    for district in districts:\n",
    "        newColumns.append('District' + district)\n",
    "        df['District' + district] = df['PdDistrict'].apply(\n",
    "            lambda x: int(x == district))\n",
    "\n",
    "    return df, newColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recoding the Address field into useful features\n",
    "The original address field is simply a string that looks like \"2000 Block of THOMAS AV\" or  \"JEFFERSON ST / HYDE ST\". In order to make this field useful there are a couple different methods we used. The first thing we did was create a flag indicatin whether the address was an intersection of 2 streets or simply a block. We also pulled out the block number (if applicable) as well as the name(s) of the street(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodeAddresses(df):\n",
    "    '''This function will attempt to create some features related to the address field in \n",
    "    the database. To do this, first, we need to split up the address field into two different\n",
    "    steet fields, a block nnumber, and a boolean specifying whether it's a street corner '''\n",
    "    \n",
    "        \n",
    "    #Also add the \"did the crime occur on a street corner field?\"\n",
    "    df['StreetCornerFlag'] = df['Address'].apply(lambda x: len(x.split(\" / \")) > 1)\n",
    "    \n",
    "    #If there are two streets, split fields. Also extract the block number\n",
    "    df['street1'] = df['Address'].apply(lambda x: re.sub(r'^\\d+ Block of ','',x.split(\" / \")[0]))\n",
    "    df['street2'] = df['Address'].apply(lambda x: (x.split(\" / \")[1]) if (len(x.split(\" / \")) > 1) else '')\n",
    "\n",
    "    df['BlockNumber'] = df['Address'].apply(lambda x: int(re.findall(r'^\\d+',x)[0]) if (len(re.findall(r'^\\d+',x)) > 0) else None )\n",
    "    df['BlockNumber'] = df['BlockNumber'].fillna(-1)\n",
    "\n",
    "    \n",
    "    streetColumns = []\n",
    "    \n",
    "    #one-hot encoding the streets requires more RAM than the standard computer has\n",
    "    if isPowerful: \n",
    "        print \"starting street dummy creation\"\n",
    "\n",
    "        #create a one-hot encoding of the Streets\n",
    "        street1Dummy = pd.get_dummies(df['street1'])\n",
    "        print \"completed street 1 dummy creation\"\n",
    "        \n",
    "        street2Dummy = pd.get_dummies(df['street2'])\n",
    "        print \"completed street 2 dummy creation\"\n",
    "\n",
    "        #turn the 0s into NaNs so that 'combine_first' can merge them\n",
    "        street1Dummy = street1Dummy.replace(0, np.nan)\n",
    "        street2Dummy = street2Dummy.replace(0, np.nan)\n",
    "        \n",
    "        #merge the 2 one-hot address frames into 1\n",
    "        mergedStreetDummy = street1Dummy.combine_first(street2Dummy)\n",
    "        print \"completed address dummy DataFrames merge\"\n",
    "        \n",
    "        #turn the NaNs back into 0s\n",
    "        mergedStreetDummy = mergedStreetDummy.fillna(0)\n",
    "        print \"completed fillna on mergedAddressDummy\"\n",
    "        \n",
    "        #extract all the new street columns\n",
    "        streetColumns = list(mergedStreetDummy.columns.values)\n",
    "\n",
    "        #merge the street data and the original dataframe\n",
    "        df = pd.concat([df, mergedStreetDummy], axis=1)\n",
    "        print \"completed merge of original df and new dummy variable df\"\n",
    "    \n",
    "    return df, ['StreetCornerFlag', 'BlockNumber'], streetColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Daily Weather Data\n",
    "We were interested to know if the weather played a role in the types of crimes that occured so we added daily information about the max/min temperature as well as percipitation. The data came from NOAA (National Oceanic and Atmospheric Administration) and was pulled from https://www.ncdc.noaa.gov/cdo-web/search. Specifically it came from the [downtown station](http://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USW00023272/detail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addWeather(df):\n",
    "    '''add 'PRCP' (precipitation),'TMAX' (Max Temperature),'TMIN' (Min Temperature) to each data point'''\n",
    "    \n",
    "    # create column to merge the weather data on (eg. 01-29-2016 becomes \"20160129\")\n",
    "    df['DATE'] = df['DateTime'].apply(lambda x: int( str(x.year)+x.strftime('%m')+x.strftime('%d') ))\n",
    "    \n",
    "    weatherData = pd.read_csv('weather1.csv')\n",
    "    \n",
    "    #replace how NaNs are encoded\n",
    "    weatherData = weatherData.replace('-9999', np.nan)\n",
    "    \n",
    "    #get subset of full dataframe\n",
    "    weatherData = weatherData[['DATE','PRCP','TMAX','TMIN']]\n",
    "    \n",
    "    #merge the data frames based on the integer coumln \"DATE\"\n",
    "    df = pd.merge(df, weatherData, on='DATE')\n",
    "    \n",
    "    return df, ['PRCP','TMAX','TMIN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recode the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crimeData, addedColumns, streetColumns = recodeData(\n",
    "    crimeData, isTrain = True)\n",
    "crimeData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Test our Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
