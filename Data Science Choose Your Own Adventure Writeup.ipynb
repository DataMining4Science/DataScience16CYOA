{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science: Choose Your Own Adventure\n",
    "**By Mac-I and Sophia**\n",
    "\n",
    "For this project, we decided to use the [San Franscisco Crime Dataset](https://www.kaggle.com/c/sf-crime), with the goal of predicting the category of crime based on the date/time of report, and the location of the report. This notebook will serve as a writeup of the work that we have done on this project, in both data exploration and building a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Before we get started, let's talk about what we're trying to do and what information we actually have!\n",
    "\n",
    "In the dataset, the information we have is:\n",
    "* **Dates**\n",
    "* Category\n",
    "* **Description**\n",
    "* **Day of Week**\n",
    "* **Police District**\n",
    "* Resolution\n",
    "* **Addresss**\n",
    "* **X (Longitude)**\n",
    "* **Y (Latitude)**\n",
    "\n",
    "The bolded items are the ones that occur in both the test and training datasets. In other words, the bolded items are the ones that we will be using to predict the cateogry of the crime. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Everything\n",
    "To keep our code neat, let's import all the helper libraries we need up here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import shapefile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "from time import time\n",
    "\n",
    "#data exploration imports\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib import cm\n",
    "from datetime import datetime\n",
    "from ipywidgets import widgets  \n",
    "from IPython.display import display\n",
    "\n",
    "import seaborn as sns#Building and testing model iputs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "\n",
    "isPowerful = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Before we get into building a model, we're going to start by just exploring the dataset. The goal of this is to just explore what kinds of relationships exist in the dataset. \n",
    "\n",
    "First, let's start by reading in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crimeData = pd.read_csv('train.csv')\n",
    "crimeData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've read in the data, we can see that we have a timestamp column. This, however is a string, so let's actually decompose this into year, month, day, and hour values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crimeData['DateTime'] = crimeData['Dates'].apply(\n",
    "    lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "crimeData['Year'] = crimeData['DateTime'].apply(lambda x: x.year)\n",
    "crimeData['Month'] = crimeData['DateTime'].apply(lambda x: x.month)\n",
    "crimeData['Day'] = crimeData['DateTime'].apply(lambda x: x.day)\n",
    "crimeData['Hour'] = crimeData['DateTime'].apply(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, we have two different kinds of data: location data and time data that we can use to predict the type of crime.\n",
    "\n",
    "First, we'll start by creating plots of the crimes for each location. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Relationship between Location and Crime Category\n",
    "In the graphs below, we'll show some of the work we did to explore how location and the category of crime are related. \n",
    "\n",
    "To do this, we're going to use ipython notebook widgets, to allow a user to choose a category and then display all the crimes of that category that occurred in our training data. \n",
    "\n",
    "First, we will create the dataframe that we need to plot the information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Only get the non lat-long-outlier crime reports\n",
    "displayCrimeData = crimeData[(crimeData.X<-121) & (crimeData.Y<40)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a function that given a crime, plots the crimes of that category. (Thanks to our [classmates](https://github.com/BrennaManning/DataScience16CYOA/blob/master/data_exploration.ipynb) for providing the idea to do a hexbin plot!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crime_map_display(crime):\n",
    "    #Load in the map data and set the appropriate lat long variables\n",
    "    mapdata = np.loadtxt(\"sf_map_copyright_openstreetmap_contributors.txt\")\n",
    "    asp = mapdata.shape[0] * 1.0 / mapdata.shape[1]\n",
    "    clipsize = [[-122.5247, -122.3366],[ 37.699, 37.8299]]\n",
    "    lon_lat_box=[-122.52469, -122.33663, 37.69862, 37.82986]\n",
    "    \n",
    "    #get only the crimes that of the category we are showing. \n",
    "    crimeDataS = displayCrimeData[displayCrimeData.Category == crime]\n",
    "    plt.figure()\n",
    "    plt.grid(False)\n",
    "    #ax = sns.kdeplot(crimeDataS.Xok, crimeDataS.Yok, clip=clipsize, aspect=1/asp)\n",
    "\n",
    "    plt.imshow(mapdata, cmap=plt.get_cmap('gray'), \n",
    "                  extent=lon_lat_box, \n",
    "                  aspect=asp)\n",
    "    \n",
    "    plt.hexbin(crimeDataS.X, crimeDataS.Y, gridsize=100,\n",
    "           extent=lon_lat_box, alpha=0.5, cmap=plt.get_cmap('Blues'), bins='log')\n",
    "    \n",
    "#     g = sns.jointplot(crimeDataS['Latitude'], crimeDataS['Longitude'], kind=\"hex\")\n",
    "#     g = sns.regplot(x=\"Latitude\", y=\"Longitude\", data=crimeDataS, fit_reg=False, scatter_kws={'alpha':0.3})\n",
    "    plt.title(crime)\n",
    "    cb = plt.colorbar()\n",
    "    cb.set_label('log10(Number of Crimes)')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will want to actually call this function when we update a widget. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = list(zip(crimeData.Category.unique(), crimeData.Category.unique()))\n",
    "crime = widgets.Select(options=categories, description='Select one of the categories:')\n",
    "widgets.interact(crime_map_display, crime = crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, as opposed to using just the number of crimes that occurred, we are using the log<sub>10</sub> of the number of crimes that occurred. If we do not do this, we see most of the cells as clear and one or two as dark blue. Plotting the log<sub>10</sub> of the number of crimes allows us to more clearly see how the number of crimes changes. \n",
    "\n",
    "In these graphs (which load a little slowly), There is notably one patch of land where crimes rarely occurr-- according to our research, this is a relatively nice park. Additionally, crimes, for the most part, tend to be concentrated in the downtown area. \n",
    "\n",
    "Notably, it appears that the \"other\" category, which are mostly traffic violations are, unsurprisingly, concentrated on major roads. \n",
    "\n",
    "One contrast to crimes being more and more concentrated downtown is the heatmap of prostitution crimes. These tend to just have two centers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give us a little more insight into what is happening, and to break up the data a little more, we decided to split this plot up by hour, too, and explore the data per category, per hour. Here we follow the same structure as above (creating a function that plots the data based on our filtered variables, and then calling that function on change of ipython notebook widgets). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def image_display(crime, time):\n",
    "    \n",
    "    #Load in the Map Data\n",
    "    mapdata = np.loadtxt(\"sf_map_copyright_openstreetmap_contributors.txt\")\n",
    "    asp = mapdata.shape[0] * 1.0 / mapdata.shape[1]\n",
    "    clipsize = [[-122.5247, -122.3366],[ 37.699, 37.8299]]\n",
    "    lon_lat_box=[-122.52469, -122.33663, 37.69862, 37.82986]\n",
    "    \n",
    "    crimeDataS = crimeData[crimeData.Category == crime][crimeData.Hour==time]\n",
    "    plt.figure()\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.imshow(mapdata, cmap=plt.get_cmap('gray'), \n",
    "                  extent=lon_lat_box, \n",
    "                  aspect=asp)\n",
    "    \n",
    "    plt.hexbin(crimeDataS.X, crimeDataS.Y, gridsize=100,\n",
    "           extent=lon_lat_box, alpha=0.5, cmap=plt.get_cmap('Blues'), bins='log')\n",
    "    \n",
    "    plt.title(crime + \" at time :\" + str(time))\n",
    "    cb = plt.colorbar()\n",
    "    cb.set_label('log10(Number of Crimes)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vals = list(zip(crimeData.Category.unique(), crimeData.Category.unique()))\n",
    "crime = widgets.Select(options=vals, description='Select one of the values:')\n",
    "time = widgets.IntSlider(min=0, max=23, value=2003)\n",
    "widgets.interact(image_display, crime = crime, time=time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some noteable takeaways here are that:\n",
    "* For most categories of crime, there is a lull around 3 am\n",
    "* Most crime tends to be concentrated downtown. We predict that this is because many more people work/live in this area than anywhere else, not because this is actually a more dangerous place to live"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is helpful to see the map for different categories, for different hours, this view of the data does not provide the most intuitive way to visualize time-based crime patterns. \n",
    "\n",
    "### Crime Distribution of Day of Week and Hour\n",
    "To dig into this, let's look first at the distribution of crime patterns by day of week and hour of the day. Using the same pattern described above, we'll implement a function that creates a plot and declare the widgets to control it.\n",
    "\n",
    "First, let's group the crimes by day of week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupedByDayOfWeek = crimeData.groupby(['DayOfWeek', 'Category']).count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can declare our function and create the widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def image_display_day_of_week(i):\n",
    "    # Get the string for the day of week\n",
    "    dayOfWeek = ['Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday']\n",
    "    day = dayOfWeek[i]\n",
    "    \n",
    "    #Get the number of crimes that occurred on this day of week\n",
    "    #(For normalization purposes)\n",
    "    totalCrimes = sum(groupedByDayOfWeek[groupedByDayOfWeek.DayOfWeek == day]['Dates'].tolist())\n",
    "    \n",
    "    #Get a list of the different crime types\n",
    "    crimeTypes = sorted(crimeData.Category.unique().tolist())\n",
    "    #Calculate the percentage of crimes that occurred that were of a given category\n",
    "    crimeCountsPercent = []\n",
    "    for crime in crimeTypes:\n",
    "        countList = groupedByDayOfWeek[(groupedByDayOfWeek.DayOfWeek == day) & (groupedByDayOfWeek.Category == crime)]['Dates'].tolist()\n",
    "        #Here, we're doing some error handling \n",
    "        #If no crimes of a certain type occurred on a given day, \n",
    "        #then append zero\n",
    "        if (len(countList) > 0):\n",
    "            count = countList[0]\n",
    "        else:\n",
    "            count = 0\n",
    "\n",
    "        crimeCountsPercent.append(1.0*count/totalCrimes)\n",
    "\n",
    "    #Create the figure\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.bar([x + 0.1 for x in range(len(crimeCountsPercent))], crimeCountsPercent, width = 0.8)\n",
    "    plt.xticks([x + 0.5 for x in range(len(crimeCountsPercent))], crimeTypes, rotation='vertical')\n",
    "    plt.axis([-0.5, 39.5, 0 ,0.3])\n",
    "    plt.title('Crime Breakdown where day of week is ' + str(day))\n",
    "    plt.xlabel('Type of Crime')\n",
    "    plt.ylabel('Percentage of Crimes')\n",
    "\n",
    "#Outside of the function create the widgets\n",
    "step_slider = widgets.IntSlider(min=0, max=6, value=0)\n",
    "widgets.interact(image_display_day_of_week, i=step_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a slightly easier way of seeing that for all days of the week, larceny/theft and other offenses are the most common crimes. \n",
    "\n",
    "In general, it appears that there is an increase of crimes like larceny/theft, assault, and drug/narcotic-related offenses over the weekend, and less during the week.\n",
    "\n",
    "Additionally, to explore whether certain crimes happen more often at certain times of day, let's also make a similar plot for hour of the day. First, we group by hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupedByHour = crimeData.groupby(['Hour', 'Category']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def image_display_hour(i):\n",
    "    #get the hour of the day\n",
    "    hour = i\n",
    "    #Count the number of crimes that occured in that hour\n",
    "    #(for normalization)\n",
    "    totalCrimes = sum(groupedByHour[groupedByHour.Hour == hour]['Dates'].tolist())\n",
    "    \n",
    "    #Get the number of each type of crime that occured in that hour\n",
    "    crimeTypes = sorted(crimeData.Category.unique().tolist())\n",
    "    crimeCountsPercent = []\n",
    "    for crime in crimeTypes:\n",
    "        countList = groupedByHour[(groupedByHour.Hour == hour) & (groupedByHour.Category == crime)]['Dates'].tolist()\n",
    "        #Handle the zero-occurrence case\n",
    "        if (len(countList) > 0):\n",
    "            count = countList[0]\n",
    "        else:\n",
    "            count = 0\n",
    "\n",
    "        crimeCountsPercent.append(1.0*count/totalCrimes)\n",
    "\n",
    "    #create the plot\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.bar([x + 0.1 for x in range(len(crimeCountsPercent))], crimeCountsPercent, width = 0.8)\n",
    "    plt.xticks([x + 0.5 for x in range(len(crimeCountsPercent))], crimeTypes, rotation='vertical')\n",
    "    plt.axis([-0.5, 39.5, 0 ,0.3])\n",
    "    plt.title('Crime Breakdown where hour = ' + str(hour))\n",
    "    plt.xlabel('Type of Crime')\n",
    "    plt.ylabel('Percentage of Crimes')\n",
    "\n",
    "\n",
    "#Create the widget\n",
    "step_slider = widgets.IntSlider(min=0, max=23, value=0)\n",
    "widgets.interact(image_display_hour, i=step_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the distribution of crimes does change over the course of the day.\n",
    "\n",
    "There is a huge shift in the distribution to be mostly larceny/theft crimes around 6pm. We hypothesize that this is because most larcenty/theft is report when people get home or to their cars after work. \n",
    "\n",
    "Additionally, at around 2 am, it appears that the distribution shifts so that assault is the most common crime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Patterns of Occurrence for Crimes\n",
    "Although these visualizations help us get a general  sense about what the distribution of crimes look like at various times, it would also be helpful to visualize the pattern of one crime over time. \n",
    "\n",
    "To do this, let's again use ipython widgets. Because this is a little computationally intensive, though, let's create the dataframe outside of the function where we update the plot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupedByTime = crimeData.groupby(['DayOfWeek', 'Hour', 'Category']).count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can write a function to plot the time heatmap for a given category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_time_heatmap(crime):\n",
    "    daysOfWeek = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "    daysOfWeekDisp = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "    hours = range(24)\n",
    "    \n",
    "    numCrimes = np.zeros((len(hours), len(daysOfWeek)))\n",
    "    for i,hour in enumerate(hours):\n",
    "        for j,dayOfWeek in enumerate(daysOfWeek):\n",
    "            try:\n",
    "                crimeCount = groupedByTime[(groupedByTime.DayOfWeek == dayOfWeek) & \n",
    "                                           (groupedByTime.Hour == hour) & \n",
    "                                           (groupedByTime.Category == crime)]['Dates'].tolist()[0]\n",
    "            except:\n",
    "                crimeCount = 0\n",
    "\n",
    "\n",
    "            numCrimes[23-hour][j] = int(crimeCount)\n",
    "\n",
    "    g = sns.heatmap(numCrimes, annot=True, fmt='.0f')\n",
    "    g.set_title(crime)\n",
    "    g.set(xticklabels = daysOfWeekDisp)\n",
    "    g.set(yticklabels = hours)\n",
    "    \n",
    "    \n",
    "crimeCategories = groupedByTime.Category.unique().tolist()\n",
    "vals = list(zip(crimeData.Category.unique(), crimeData.Category.unique()))\n",
    "crime = widgets.Select(options=vals, description='Select one of the values:')\n",
    "widgets.interact(show_time_heatmap, crime = crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In exploring these plots, it appears that there tends to be a similar pattern of higher levels of crime in the evening and lower levels of crime in the morning. Many crimes also occur more often on the weekends. \n",
    "\n",
    "Notable exceptions to this pattern appear to be drug/narcotic-related offenses (which are most common Wednesdays around lunch time) and missing person cases which appear to be reported most often when in the mornings when people don't show up for work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Model\n",
    "Now that we've explored the data, we're going to start building a model. To make our lives easier, we're going to follow a pretty simple workflow. We will \n",
    "1. Read in the Data\n",
    "2. Clean the training data\n",
    "3. Create a Model using the cleaned data\n",
    "4. Score the model using the crime categories in the training data\n",
    "5. If the model performs well (better than previous attempts) we will: \n",
    "  1. Repeat steps 1-3 with the test data. \n",
    "  2. Generate a submission file to upload to Kaggle. \n",
    "  \n",
    "Rather than talking through each successive iteration of our model, the following code will instead talk through everything we developed in each of these steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data and constants\n",
    "Here we load the data into a panda dataframe and set some initial constants. Specifically, we are hard-coding a variable with all of the different categories. This will come in handy later when we generate a submission file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crimeData = pd.read_csv('train.csv')\n",
    "#The categories found in the dataset\n",
    "Categories = ['ARSON', 'ASSAULT', 'BAD CHECKS', 'BRIBERY', 'BURGLARY',\n",
    "              'DISORDERLY CONDUCT', 'DRIVING UNDER THE INFLUENCE',\n",
    "              'DRUG/NARCOTIC', 'DRUNKENNESS', 'EMBEZZLEMENT', 'EXTORTION',\n",
    "              'FAMILY OFFENSES', 'FORGERY/COUNTERFEITING', 'FRAUD', 'GAMBLING',\n",
    "              'KIDNAPPING', 'LARCENY/THEFT', 'LIQUOR LAWS', 'LOITERING',\n",
    "              'MISSING PERSON', 'NON-CRIMINAL', 'OTHER OFFENSES',\n",
    "              'PORNOGRAPHY/OBSCENE MAT', 'PROSTITUTION', 'RECOVERED VEHICLE',\n",
    "              'ROBBERY', 'RUNAWAY', 'SECONDARY CODES', 'SEX OFFENSES FORCIBLE',\n",
    "              'SEX OFFENSES NON FORCIBLE', 'STOLEN PROPERTY', 'SUICIDE',\n",
    "              'SUSPICIOUS OCC', 'TREA', 'TRESPASS', 'VANDALISM', 'VEHICLE THEFT',\n",
    "              'WARRANTS', 'WEAPON LAWS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "This section will cover how we cleaned the data as well as how we added features. In this section, we first define a wrapper function that will take in a dataframe that we read in from a csv and return our pre-processed dataframe. This function will call the functions that we write to clean/recode/generate new features for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodeData(df, isTrain = False):\n",
    "    '''This function takes in the dataframe that we get from loading in the \n",
    "    SF crime data and returns a re-coded dataframe that has all the \n",
    "    additional features we want to add and the categorical features recoded \n",
    "    and cleaned.\n",
    "    '''\n",
    "\n",
    "    #All of these functions return both the new dataframe ad the list of columns that we added. \n",
    "    df, newLatLon = removeOutlierLatLon(df)\n",
    "    df, newDate = recodeDates(df)\n",
    "    df, newDistrict = recodePoliceDistricts(df)\n",
    "    df, newAddress, streetColumns = recodeAddresses(df)\n",
    "    df, newWeather = addWeather(df)\n",
    "\n",
    "    #Add the new columns to our list of added columns\n",
    "    addedColumns = [] \n",
    "    addedColumns += newDate\n",
    "    addedColumns += newDistrict \n",
    "    addedColumns += newLatLon\n",
    "    addedColumns += newAddress\n",
    "    addedColumns += newWeather\n",
    "   \n",
    "\n",
    "    #If this is the traning data, we want to remove the columns that we will not have access to in the test set. \n",
    "    #We also want to recode the crime category information in the dataframe if this is the test dataset.\n",
    "    if (isTrain):\n",
    "        newCategory = recodeCategories(df)\n",
    "        addedColumns += newCategory\n",
    "        try: #prevents error if the columns have already been removed\n",
    "            columnsToDrop = ['Descript', 'Resolution']\n",
    "            df.drop(columnsToDrop, axis=1, inplace=True)\n",
    "        except:\n",
    "            print \"already recoded\"\n",
    "         \n",
    "\n",
    "    return df, addedColumns, streetColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recode the Categories\n",
    "Here we turn the category names into integers to ease classification. Not all of the models that we used can handle text-based category data, so we need to convert the categories to a number. We do this by mapping each category to its respective index in the category list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodeCategories(df):\n",
    "    '''This function will recode the Categories from strings into integers'''\n",
    "    df['CategoryRecode'] = df.Category.apply(lambda x: Categories.index(x))\n",
    "        \n",
    "    return df, ['CategoryRecode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing the Latitudes and Longitudes that do fall in San Francisco\n",
    "During our data exploration we noticed that some of the latitudes and longitudes listed were not anywhere close to San Francisco. In order to fix this we calculated the median latidute and logitude for each police district. We then assigned the appropriate median latitude and logitude to those data points with invalid latitude and logitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeOutlierLatLon(df):\n",
    "    '''This function will remove outlier Latitudes and Longitudes'''\n",
    "    df.loc[df.X > -121, 'X'] = df.loc[(df.X > -121)].apply(lambda row: df.X[df[\"PdDistrict\"] == row['PdDistrict']].median(), axis=1)\n",
    "    df.loc[df.Y > 38, 'Y'] = df.loc[(df.Y > 38)].apply(lambda row: df.Y[df[\"PdDistrict\"] == row['PdDistrict']].median(), axis=1)\n",
    "\n",
    "    return df, ['X', 'Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recoding Dates\n",
    "In order to make the \"Dates\" column useful we needed to recode them into columns such as \"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\". We also needed to recode the \"DayOfWeek\" into a nurerical format since some of the models can't handle string categorical data. It also makes sense from the perspective that a model may want to group data by weekday vs weekend which is much easier with numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodeDates(df):\n",
    "    '''This function takes in a dataframe and recodes the date field into \n",
    "    useable values. Here, we also recode the day of week.'''\n",
    "    #Recode the dates column to year, month, day and hour columns\n",
    "    df['DateTime'] = pd.to_datetime(df['Dates'], format ='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    df['Year'] = df['DateTime'].apply(lambda x: x.year)\n",
    "    df['Month'] = df['DateTime'].apply(lambda x: x.month)\n",
    "    df['Day'] = df['DateTime'].apply(lambda x: x.day)\n",
    "    df['Hour'] = df['DateTime'].apply(lambda x: x.hour)\n",
    "    df['Minute'] = df['DateTime'].apply(lambda x: x.minute)\n",
    "    df['DayOfWeekRecode'] = df['DateTime'].apply(lambda x: x.weekday())\n",
    "\n",
    "    return df, ['Year', 'Month', 'Day', 'Hour', 'Minute', 'DayOfWeekRecode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recoding the Police districts\n",
    "Similarly to the \"DayOfWeek\" column, the \"PdDistrict\" column needed to be recoded in order to be useful. We did this with one-hot encoding since the there is no inherent order to the districts, unlike day of week, where there is an order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodePoliceDistricts(df):\n",
    "    '''This function recodes the police district to a one-hot encoding scheme.'''\n",
    "    districts = df['PdDistrict'].unique().tolist()\n",
    "    \n",
    "    dummies = pd.get_dummies(df['PdDistrict'], prefix=\"PdDistrict\")\n",
    "    \n",
    "    newColumns = dummies.columns.tolist()\n",
    "    print newColumns\n",
    "    \n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    return df, newColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recoding the Address field into useful features\n",
    "The original address field is simply a string that looks like \"2000 Block of THOMAS AV\" or  \"JEFFERSON ST / HYDE ST\". In order to make this field useful there are a couple different methods we used. The first thing we did was create a flag indicatin whether the address was an intersection of 2 streets or simply a block. We also pulled out the block number (if applicable) as well as the name(s) of the street(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodeAddresses(df):\n",
    "    '''This function will attempt to create some features related to the address field in \n",
    "    the database. To do this, first, we need to split up the address field into two different\n",
    "    steet fields, a block nnumber, and a boolean specifying whether it's a street corner '''\n",
    "    \n",
    "        \n",
    "    #Also add the \"did the crime occur on a street corner field?\"\n",
    "    df['StreetCornerFlag'] = df['Address'].apply(lambda x: len(x.split(\" / \")) > 1)\n",
    "    \n",
    "    #If there are two streets, split fields. Also extract the block number\n",
    "    df['street1'] = df['Address'].apply(lambda x: re.sub(r'^\\d+ Block of ','',x.split(\" / \")[0]))\n",
    "    df['street2'] = df['Address'].apply(lambda x: (x.split(\" / \")[1]) if (len(x.split(\" / \")) > 1) else '')\n",
    "\n",
    "    df['BlockNumber'] = df['Address'].apply(lambda x: int(re.findall(r'^\\d+',x)[0]) if (len(re.findall(r'^\\d+',x)) > 0) else None )\n",
    "    df['BlockNumber'] = df['BlockNumber'].fillna(-1)\n",
    "\n",
    "    \n",
    "    streetColumns = []\n",
    "    \n",
    "    #one-hot encoding the streets requires more RAM than the standard computer has\n",
    "    if isPowerful: \n",
    "        print \"starting street dummy creation\"\n",
    "\n",
    "        #create a one-hot encoding of the Streets\n",
    "        street1Dummy = pd.get_dummies(df['street1'])\n",
    "        print \"completed street 1 dummy creation\"\n",
    "        \n",
    "        street2Dummy = pd.get_dummies(df['street2'])\n",
    "        print \"completed street 2 dummy creation\"\n",
    "\n",
    "        #turn the 0s into NaNs so that 'combine_first' can merge them\n",
    "        street1Dummy = street1Dummy.replace(0, np.nan)\n",
    "        street2Dummy = street2Dummy.replace(0, np.nan)\n",
    "        \n",
    "        #merge the 2 one-hot address frames into 1\n",
    "        mergedStreetDummy = street1Dummy.combine_first(street2Dummy)\n",
    "        print \"completed address dummy DataFrames merge\"\n",
    "        \n",
    "        #turn the NaNs back into 0s\n",
    "        mergedStreetDummy = mergedStreetDummy.fillna(0)\n",
    "        print \"completed fillna on mergedAddressDummy\"\n",
    "        \n",
    "        #extract all the new street columns\n",
    "        streetColumns = list(mergedStreetDummy.columns.values)\n",
    "\n",
    "        #merge the street data and the original dataframe\n",
    "        df = pd.concat([df, mergedStreetDummy], axis=1)\n",
    "        print \"completed merge of original df and new dummy variable df\"\n",
    "    \n",
    "    return df, ['StreetCornerFlag', 'BlockNumber'], streetColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Daily Weather Data\n",
    "We were interested to know if the weather played a role in the types of crimes that occured so we added daily information about the max/min temperature as well as percipitation. The data came from NOAA (National Oceanic and Atmospheric Administration) and was pulled from https://www.ncdc.noaa.gov/cdo-web/search. Specifically it came from the [downtown station](http://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USW00023272/detail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addWeather(df):\n",
    "    '''add 'PRCP' (precipitation),'TMAX' (Max Temperature),'TMIN' (Min Temperature) to each data point'''\n",
    "    \n",
    "    # create column to merge the weather data on (eg. 01-29-2016 becomes \"20160129\")\n",
    "    df['DATE'] = df['DateTime'].apply(lambda x: int( str(x.year)+x.strftime('%m')+x.strftime('%d') ))\n",
    "    \n",
    "    weatherData = pd.read_csv('weather1.csv')\n",
    "    \n",
    "    #replace how NaNs are encoded\n",
    "    weatherData = weatherData.replace('-9999', np.nan)\n",
    "    \n",
    "    #get subset of full dataframe\n",
    "    weatherData = weatherData[['DATE','PRCP','TMAX','TMIN']]\n",
    "    \n",
    "    #merge the data frames based on the integer coumln \"DATE\"\n",
    "    df = pd.merge(df, weatherData, on='DATE')\n",
    "    \n",
    "    return df, ['PRCP','TMAX','TMIN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recode the Training Data\n",
    "Now that we have created functions that do all of our pre-processing for us, we recode the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crimeData, addedColumns, streetColumns = recodeData(\n",
    "    crimeData, isTrain = True)\n",
    "crimeData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Test our Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and select the data for training\n",
    "Here we specify which predictors we intend to use to train our model ('columnsToUse'). In the code block below we list some of the most interesting sets of predictors. In order to specify which one to use simply leave that line uncommented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#use all of the columns added from our preprocessing of the data\n",
    "#columnsToUse = addedColumns\n",
    "\n",
    "#Use just the basic columns (date, time, and location)\n",
    "columnsToUse = ['X','Y', 'Year', 'Month', 'Day','Hour', 'Minute',\n",
    "        'PdDistrict_BAYVIEW', 'PdDistrict_CENTRAL', 'PdDistrict_INGLESIDE', \n",
    "        'PdDistrict_MISSION', 'PdDistrict_NORTHERN', 'PdDistrict_PARK', 'PdDistrict_RICHMOND', \n",
    "        'PdDistrict_SOUTHERN', 'PdDistrict_TARAVAL', 'PdDistrict_TENDERLOIN']\n",
    "\n",
    "# Use just the basic columns and whether the crimes were reported to be on a street corner\n",
    "# columnsToUse = ['X','Y', 'Year', 'Month', 'Day','Hour', 'Minute',\n",
    "#        'DayOfWeekRecode', 'PdDistrict_BAYVIEW', 'PdDistrict_CENTRAL', 'PdDistrict_INGLESIDE', \n",
    "#         'PdDistrict_MISSION', 'PdDistrict_NORTHERN', 'PdDistrict_PARK', 'PdDistrict_RICHMOND', \n",
    "#         'PdDistrict_SOUTHERN', 'PdDistrict_TARAVAL', 'PdDistrict_TENDERLOIN', 'StreetCornerFlag']\n",
    "\n",
    "#use the basic columns, whether the crime was on a corner, and the one-hot encoding of the most common streets\n",
    "#columnsToUse = ['X','Y', 'Year', 'Month', 'Day','Hour', 'Minute',\n",
    "#        'DayOfWeekRecode', 'PdDistrict_BAYVIEW', 'PdDistrict_CENTRAL', 'PdDistrict_INGLESIDE', \n",
    "#         'PdDistrict_MISSION', 'PdDistrict_NORTHERN', 'PdDistrict_PARK', 'PdDistrict_RICHMOND', \n",
    "#         'PdDistrict_SOUTHERN', 'PdDistrict_TARAVAL', 'PdDistrict_TENDERLOIN', 'StreetCornerFlag'] + commonStreets\n",
    "\n",
    "#use the basic columns, whether the crime was on a corner, and the daily weather\n",
    "#columnsToUse = ['X','Y', 'Year', 'Month', 'Day','Hour', 'Minute',\n",
    "#        'DayOfWeekRecode', 'PdDistrict_BAYVIEW', 'PdDistrict_CENTRAL', 'PdDistrict_INGLESIDE', \n",
    "#         'PdDistrict_MISSION', 'PdDistrict_NORTHERN', 'PdDistrict_PARK', 'PdDistrict_RICHMOND', \n",
    "#         'PdDistrict_SOUTHERN', 'PdDistrict_TARAVAL', 'PdDistrict_TENDERLOIN', 'StreetCornerFlag', 'PRCP','TMAX','TMIN']\n",
    "\n",
    "X = crimeData[columnsToUse]\n",
    "y = crimeData['CategoryRecode']\n",
    "\n",
    "print \"prepping for training complete\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create function to compute the logloss cross-validation score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runOnTrainData(clf, X, y, numSplits=3):\n",
    "    '''This function takes in a classifier and the number of folds to compute the cross-validation \n",
    "    score. It then splits the data into multiple training and test sets. For each split the model \n",
    "    is trained on the training data and then the logloss score is calculated based on the predictions \n",
    "    generated by the model.'''\n",
    "    \n",
    "    #split the data into training and test sets while ensuring that every category appears in both sets\n",
    "    k_folds = StratifiedShuffleSplit(y, numSplits, test_size=0.5, random_state=0)\n",
    "\n",
    "    #create list to store logloss scores in\n",
    "    scores = []\n",
    "    \n",
    "    print \"starting kfold testing\"\n",
    "    #enumerate through all the folds\n",
    "    for k, (train, test) in enumerate(k_folds):\n",
    "        print \"\"\n",
    "        print \"starting fit: \" + str(k) + \" of \" + str(numSplits)\n",
    "        start = time()\n",
    "        clf.fit(X.iloc[train], y.iloc[train])\n",
    "        print \"fit complete, time: \" + str((time() - start))\n",
    "        startPredictTime = time()\n",
    "        probs = clf.predict_proba(X.iloc[test])\n",
    "        print \"predict complete, time: \" + str((time() - startPredictTime))\n",
    "        score = log_loss(y.iloc[test].values, probs)\n",
    "        print \"Logloss score: \" + str(score)\n",
    "        print \"total time: \" + str((time() - start))\n",
    "        scores.append(score)\n",
    "\n",
    "    print \"\"\n",
    "    print(scores)\n",
    "    print(\"Average score: \" + str(np.average(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our First Model: Random Forest\n",
    "We chose a random forest classifier as our first model because we wanted to learn more about them. Additionally, Random forests seemed to be the most common method on Kaggle. We chose the initial model parameters based the maximum values our computers could handle and included all of the features we had generated thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columnsToUse = ['X','Y', 'Year', 'Month', 'Day','Hour', 'Minute',\n",
    "       'DayOfWeekRecode', 'PdDistrict_BAYVIEW', 'PdDistrict_CENTRAL', 'PdDistrict_INGLESIDE', \n",
    "        'PdDistrict_MISSION', 'PdDistrict_NORTHERN', 'PdDistrict_PARK', 'PdDistrict_RICHMOND', \n",
    "        'PdDistrict_SOUTHERN', 'PdDistrict_TARAVAL', 'PdDistrict_TENDERLOIN']\n",
    "X = crimeData[columnsToUse]\n",
    "y = crimeData['CategoryRecode']\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=30, max_depth = 7, random_state=1, n_jobs = -1)\n",
    "runOnTrainData(clf, X, y, numSplits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our reaction\n",
    "We were happy with this score (2.491) as our first attempt because it was a significant improvement over guessing equally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Second Interation\n",
    "Our second iteration involved adding a new feature, a flag indicating whether the crime's reported address was an intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columnsToUse = ['X','Y', 'Year', 'Month', 'Day','Hour', 'Minute',\n",
    "       'DayOfWeekRecode', 'PdDistrict_BAYVIEW', 'PdDistrict_CENTRAL', 'PdDistrict_INGLESIDE', \n",
    "        'PdDistrict_MISSION', 'PdDistrict_NORTHERN', 'PdDistrict_PARK', 'PdDistrict_RICHMOND', \n",
    "        'PdDistrict_SOUTHERN', 'PdDistrict_TARAVAL', 'PdDistrict_TENDERLOIN', 'StreetCornerFlag']\n",
    "X = crimeData[columnsToUse]\n",
    "y = crimeData['CategoryRecode']\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=30, max_depth = 7, random_state=1, n_jobs = -1)\n",
    "runOnTrainData(clf, X, y, numSplits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our Reaction\n",
    "Adding the street intersection indicator improved the model's score from 2.491 to 2.453. We were pleased that adding this new feature improved our model and decided to make our first kaggle submission (see code at bottom of script). On the Kaggle testing data we scored 2.44156. This was great! It was reassuring to know that our dataset was large enough that our cross-validation scores were likley to be very simlialr to the scores on Kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our third Interation\n",
    "For our third iteration we decied to investigate our on campus resource, \"deepthought\". Deepthought is a mini \"super computer\" with 1TB of RAM and 48 cores. We adjusted our parameters to include more trees and depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if isPowerful:\n",
    "    columnsToUse = ['X','Y', 'Year', 'Month', 'Day','Hour', 'Minute',\n",
    "       'DayOfWeekRecode', 'PdDistrict_BAYVIEW', 'PdDistrict_CENTRAL', 'PdDistrict_INGLESIDE', \n",
    "        'PdDistrict_MISSION', 'PdDistrict_NORTHERN', 'PdDistrict_PARK', 'PdDistrict_RICHMOND', \n",
    "        'PdDistrict_SOUTHERN', 'PdDistrict_TARAVAL', 'PdDistrict_TENDERLOIN', 'StreetCornerFlag']\n",
    "    X = crimeData[columnsToUse]\n",
    "    y = crimeData['CategoryRecode']\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=300, max_depth = 20, random_state=1, n_jobs = 1)\n",
    "    runOnTrainData(clf, X, y, numSplits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our Reaction\n",
    "Increasing the number of trees and the maximum depth dramatically improved our score on Kaggle to 2.31827. This was quite exciting and good enough to put us in the top 100 spots on the leaderboard. While we we obviously excited, we were a bit saddened at the idea that simply throwing more computation at the problem could produce way better results than the features we had engineered thus far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our 4th iteration\n",
    "For our 4th iteration we wanted to go back and see if we could engineer new features that would improve the model. We decided to focus on the address coulm provided. For each crime we identified the street name(s) where the crime occured and ont-hot encoded them. In order to save ourselves some RAM and avoid overfitting we only looked at the most common streets for each crime category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The most common streets for each crime in the dataset\n",
    "commonStreets = ['FOLSOM ST','16TH ST','JONES ST','TAYLOR ST',\n",
    "                 'ARMSTRONG AV','EDDY ST','LARKIN ST','CASTRO ST',\n",
    "                 '10TH AV','5TH ST','HAIGHT ST','OFARRELL ST',\n",
    "                 '11TH AV','PAGE ST','FITCH ST','CAPP ST','13TH ST',\n",
    "                 '24TH AV','17TH ST','18TH ST','19TH ST','GENEVA AV',\n",
    "                 'GEARY BL','BRYANT ST','HYDE ST','4TH ST','FULTON ST',\n",
    "                 'LEAVENWORTH ST','COLE ST','ALEMANY BL','PHELPS ST',\n",
    "                 'MISSION ST','6TH ST','12TH AV','SHOTWELL ST',\n",
    "                 'TREAT AV','7TH ST','JEFFERSON ST','QUESADA AV',\n",
    "                 'TURK ST','2ND ST','MARKET ST','GGBRIDGE HY',\n",
    "                 '24TH ST','CAPITOL AV','KEARNY ST','HARRISON ST',\n",
    "                 'LYON ST','BUSH ST','POLK ST','3RD ST','ELLIS ST',\n",
    "                 'SOUTH VAN NESS AV','POTRERO AV','20TH ST','POWELL ST']\n",
    "if isPowerful:\n",
    "    columnsToUse = ['X','Y', 'Year', 'Month', 'Day','Hour', 'Minute',\n",
    "       'DayOfWeekRecode', 'PdDistrict_BAYVIEW', 'PdDistrict_CENTRAL', 'PdDistrict_INGLESIDE', \n",
    "        'PdDistrict_MISSION', 'PdDistrict_NORTHERN', 'PdDistrict_PARK', 'PdDistrict_RICHMOND', \n",
    "        'PdDistrict_SOUTHERN', 'PdDistrict_TARAVAL', 'PdDistrict_TENDERLOIN', 'StreetCornerFlag'] + commonStreets\n",
    "    X = crimeData[columnsToUse]\n",
    "    y = crimeData['CategoryRecode']\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=300, max_depth = 20, random_state=1, n_jobs = 1)\n",
    "    runOnTrainData(clf, X, y, numSplits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our Reaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our 5th Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=30, objective='multi:softprob', max_delta_step = 1, learning_rate = 1, nthread = -1)\n",
    "runOnTrainData(clf, X, y, numSplits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
