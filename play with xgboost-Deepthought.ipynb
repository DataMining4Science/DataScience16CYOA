{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib import cm\n",
    "from datetime import datetime\n",
    "from ipywidgets import widgets  \n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "print \"imports imported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crimeData = pd.read_csv('train.csv')\n",
    "Categories = ['ARSON', 'ASSAULT', 'BAD CHECKS', 'BRIBERY', 'BURGLARY',\n",
    "              'DISORDERLY CONDUCT', 'DRIVING UNDER THE INFLUENCE',\n",
    "              'DRUG/NARCOTIC', 'DRUNKENNESS', 'EMBEZZLEMENT', 'EXTORTION',\n",
    "              'FAMILY OFFENSES', 'FORGERY/COUNTERFEITING', 'FRAUD', 'GAMBLING',\n",
    "              'KIDNAPPING', 'LARCENY/THEFT', 'LIQUOR LAWS', 'LOITERING',\n",
    "              'MISSING PERSON', 'NON-CRIMINAL', 'OTHER OFFENSES',\n",
    "              'PORNOGRAPHY/OBSCENE MAT', 'PROSTITUTION', 'RECOVERED VEHICLE',\n",
    "              'ROBBERY', 'RUNAWAY', 'SECONDARY CODES', 'SEX OFFENSES FORCIBLE',\n",
    "              'SEX OFFENSES NON FORCIBLE', 'STOLEN PROPERTY', 'SUICIDE',\n",
    "              'SUSPICIOUS OCC', 'TREA', 'TRESPASS', 'VANDALISM', 'VEHICLE THEFT',\n",
    "              'WARRANTS', 'WEAPON LAWS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodeData(df, isTrain = False):\n",
    "    '''This function takes in the dataframe that we get from loading in the \n",
    "    SF crime data and returns a re-coded dataframe that has all the \n",
    "    additional features we want to add and the categorical features recoded \n",
    "    and cleaned.\n",
    "    '''\n",
    "\n",
    "    #since the modifications are done in-place we don't return the dataframe. \n",
    "    #we do, however, return the list of all the columns we added.\n",
    "    df, newLatLon = removeOutlierLatLon(df)\n",
    "    df, newDate = recodeDates(df)\n",
    "    df, newDistrict = recodePoliceDistricts(df)\n",
    "    df, newWeather = addWeather(df)\n",
    "    print \"recoding addresses\"\n",
    "    df, newAddress, streetColumns = recodeAddresses(df)\n",
    "\n",
    "    \n",
    "    addedColumns = [] \n",
    "    addedColumns += newDate\n",
    "    addedColumns += newDistrict \n",
    "    addedColumns += newLatLon\n",
    "    addedColumns += newWeather\n",
    "    addedColumns += newAddress\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    if (isTrain):\n",
    "        newCategory = recodeCategories(df)\n",
    "        addedColumns += newCategory\n",
    "        try: #prevents error if the coumns have already been removed or we are processing test data\n",
    "            columnsToDrop = ['Descript', 'Resolution']\n",
    "            df.drop(columnsToDrop, axis=1, inplace=True)\n",
    "        except:\n",
    "            print \"already recoded or using test data\"\n",
    "         \n",
    "\n",
    "    return df, addedColumns, streetColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodeDates(df):\n",
    "    '''This function takes in a dataframe and recodes the date field into \n",
    "    useable values. Here, we also recode the day of week.'''\n",
    "    #Recode the dates column to year, month, day and hour columns\n",
    "    df['DateTime'] = df['Dates'].apply(\n",
    "        lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    df['Year'] = df['DateTime'].apply(lambda x: x.year)\n",
    "    df['Month'] = df['DateTime'].apply(lambda x: x.month)\n",
    "    df['Day'] = df['DateTime'].apply(lambda x: x.day)\n",
    "    df['Hour'] = df['DateTime'].apply(lambda x: x.hour)\n",
    "    df['Minute'] = df['DateTime'].apply(lambda x: x.minute)\n",
    "    df['DayOfWeekRecode'] = df['DateTime'].apply(lambda x: x.weekday())\n",
    "    df['MinuteOfWeek'] = df['DateTime'].apply(lambda x: x.weekday()*24*60 + x.hour*60 + x.minute)\n",
    "\n",
    "    return df, ['Year', 'Month', 'Day', 'Hour', 'Minute', 'DayOfWeekRecode','MinuteOfWeek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodePoliceDistricts(df):\n",
    "    '''This function recodes the police district to a one-hot encoding \n",
    "    scheme.'''\n",
    "    districts = df['PdDistrict'].unique().tolist()\n",
    "    newColumns = []\n",
    "    for district in districts:\n",
    "        newColumns.append('District' + district)\n",
    "        df['District' + district] = df['PdDistrict'].apply(\n",
    "            lambda x: int(x == district))\n",
    "\n",
    "    return df, newColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodeAddresses(df):\n",
    "    '''This function will attempt to create some features related to the address field in the database. To do this, \n",
    "    first, we need to split up the address field into two different address fields'''\n",
    "    \n",
    "    #If there are two addresss, split fields. Also extract the block number\n",
    "    df['Address1'] = df['Address'].apply(lambda x: re.sub(r'^\\d+ Block of ','',x.split(\" / \")[0]))\n",
    "    df['Address2'] = df['Address'].apply(lambda x: (x.split(\" / \")[1]) if (len(x.split(\" / \")) > 1) else '')\n",
    "    \n",
    "    streets = set(df['Address1'].unique().tolist() + df['Address2'].unique().tolist())\n",
    "    \n",
    "    streetColumns = []\n",
    "    i = 0\n",
    "    \n",
    "    print \"starting address dummy creation\"\n",
    "    \n",
    "    #address1Dummy = pd.get_dummies(df['Address1']).rename(columns=lambda x: str(x))\n",
    "    address1Dummy = pd.get_dummies(df['Address1'])\n",
    "    address1Dummy = address1Dummy.replace(0, np.nan)\n",
    "    \n",
    "    print \"completed address 1 dummy creation\"\n",
    "    address2Dummy = pd.get_dummies(df['Address2'])\n",
    "    address2Dummy = address2Dummy.replace(0, np.nan)\n",
    "    \n",
    "    print \"completed address 2 dummy creation\"\n",
    "    \n",
    "#     print address1Dummy\n",
    "#     print address2Dummy.info()\n",
    "    \n",
    "    mergedAddressDummy = address1Dummy.combine_first(address2Dummy)\n",
    "    \n",
    "    print \"completed address dummy DataFrames merge\"\n",
    "    mergedAddressDummy = mergedAddressDummy.fillna(0)\n",
    "    print \"completed fillna on mergedAddressDummy\"\n",
    "    \n",
    "    streetColumns = list(mergedAddressDummy.columns.values)\n",
    "    \n",
    "    df = pd.concat([df, mergedAddressDummy], axis=1)\n",
    "    print \"completed merge of original df and new dummy variable df\"\n",
    "    \n",
    "    print \"Dummy creation finished\"\n",
    "    \n",
    "#     for street in streets:\n",
    "#         df['OnStreet' + street] = df.apply(lambda x: (x['Address1'] == street or x['Address2'] == street), axis=1)\n",
    "#         streetColumns.append('OnStreet' + street)\n",
    "#         i += 1\n",
    "#         print i\n",
    "        \n",
    "\n",
    "    \n",
    "    df['BlockNumber'] = df['Address'].apply(lambda x: int(re.findall(r'^\\d+',x)[0]) if (len(re.findall(r'^\\d+',x)) > 0) else None )\n",
    "    df['BlockNumber'] = df['BlockNumber'].fillna(-1)\n",
    "    \n",
    "    #Also add the \"did the crime occur on a street corner field?\"\n",
    "    df['StreetCornerFlag'] = df['Address'].apply(lambda x: len(x.split(\" / \")) > 1)\n",
    "    \n",
    "    return df, ['StreetCornerFlag', 'BlockNumber'], streetColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeOutlierLatLon(df):\n",
    "    '''This function will attempt remove outlier Latitudes and Longitudes'''\n",
    "    df.loc[df.X > -121, 'X'] = df.loc[(df.X > -121)].apply(lambda row: df.X[df[\"PdDistrict\"] == row['PdDistrict']].median(), axis=1)\n",
    "    df.loc[df.Y > 38, 'Y'] = df.loc[(df.Y > 38)].apply(lambda row: df.Y[df[\"PdDistrict\"] == row['PdDistrict']].median(), axis=1)\n",
    "\n",
    "    return df, ['X', 'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addWeather(df):\n",
    "    df['DATE'] = df['DateTime'].apply(lambda x: int( str(x.year)+x.strftime('%m')+x.strftime('%d') ))\n",
    "    \n",
    "    weatherData = pd.read_csv('weather1.csv')\n",
    "    weatherData = weatherData.replace('-9999', np.nan)\n",
    "    \n",
    "    weatherData = weatherData[['DATE','PRCP','TMAX','TMIN']]\n",
    "    \n",
    "    df = pd.merge(df, weatherData, on='DATE')\n",
    "    \n",
    "    return df, ['PRCP','TMAX','TMIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodeCategories(df):\n",
    "    '''This function will attempt remove outlier Latitudes and Longitudes'''\n",
    "    #if 'Category' in df.columns:\n",
    "    df['CategoryRecode'] = df.Category.apply(lambda x: Categories.index(x))\n",
    "        \n",
    "    return df, ['CategoryRecode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crimeData, addedColumns, streetColumns = recodeData(\n",
    "    crimeData, isTrain = True)\n",
    "crimeData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columnsToUse = addedColumns\n",
    "\n",
    "commonStreets = ['FOLSOM ST','16TH ST','JONES ST','TAYLOR ST',\n",
    "                 'ARMSTRONG AV','EDDY ST','LARKIN ST','CASTRO ST',\n",
    "                 '10TH AV','5TH ST','HAIGHT ST','OFARRELL ST',\n",
    "                 '11TH AV','PAGE ST','FITCH ST','CAPP ST','13TH ST',\n",
    "                 '24TH AV','17TH ST','18TH ST','19TH ST','GENEVA AV',\n",
    "                 'GEARY BL','BRYANT ST','HYDE ST','4TH ST','FULTON ST',\n",
    "                 'LEAVENWORTH ST','COLE ST','ALEMANY BL','PHELPS ST',\n",
    "                 'MISSION ST','6TH ST','12TH AV','SHOTWELL ST',\n",
    "                 'TREAT AV','7TH ST','JEFFERSON ST','QUESADA AV',\n",
    "                 'TURK ST','2ND ST','MARKET ST','GGBRIDGE HY',\n",
    "                 '24TH ST','CAPITOL AV','KEARNY ST','HARRISON ST',\n",
    "                 'LYON ST','BUSH ST','POLK ST','3RD ST','ELLIS ST',\n",
    "                 'SOUTH VAN NESS AV','POTRERO AV','20TH ST','POWELL ST']\n",
    "\n",
    "columnsToUse = ['X','Y', 'Year', 'Month', 'Day','Hour', 'Minute',\n",
    "       'DayOfWeekRecode', 'DistrictNORTHERN', 'DistrictPARK',\n",
    "       'DistrictINGLESIDE', 'DistrictBAYVIEW', 'DistrictRICHMOND',\n",
    "       'DistrictCENTRAL', 'DistrictTARAVAL', 'DistrictTENDERLOIN',\n",
    "       'DistrictMISSION', 'DistrictSOUTHERN', 'StreetCornerFlag', 'PRCP','TMAX','TMIN']\n",
    "\n",
    "# columnsToUse = ['X','Y', 'Year', 'Month', 'Day','Hour', 'Minute',\n",
    "#        'DayOfWeekRecode', 'StreetCornerFlag']\n",
    "\n",
    "X = crimeData[columnsToUse]\n",
    "y = crimeData['CategoryRecode']\n",
    "\n",
    "print \"prepping for training complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runOnTrainData(clf, numSplits=1):\n",
    "    \n",
    "    k_folds = StratifiedShuffleSplit(y, numSplits, test_size=0.5, random_state=0)\n",
    "\n",
    "    scores = []\n",
    "    print \"starting kfold testing\"\n",
    "\n",
    "    for k, (train, test) in enumerate(k_folds):\n",
    "        print \"starting fit\"\n",
    "        start = time()\n",
    "        clf.fit(X.iloc[train], y.iloc[train], eval_metric = 'mlogloss', verbose=3)\n",
    "        print \"fit complete, time: \" + str((time() - start))\n",
    "        startPredictTime = time()\n",
    "        probs = clf.predict_proba(X.iloc[test])\n",
    "        print \"predict complete, time: \" + str((time() - startPredictTime))\n",
    "        score = log_loss(y.iloc[test].values, probs)\n",
    "        print score\n",
    "        print \"total time: \" + str((time() - start))\n",
    "        scores.append(score)\n",
    "\n",
    "    print(scores)\n",
    "    print(\"Average: \" + str(np.average(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(max_depth=8, n_estimators=800, objective='multi:softprob', max_delta_step = .8, learning_rate = .0375, nthread = -1)\n",
    "runOnTrainData(clf, numSplits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictorsUsed = ['DistrictBAYVIEW',\n",
    " 'Y',\n",
    " 'Hour',\n",
    " 'DistrictSOUTHERN',\n",
    " 'DistrictTARAVAL',\n",
    " 'DistrictRICHMOND',\n",
    " 'Month',\n",
    " 'DistrictTENDERLOIN',\n",
    " 'DistrictNORTHERN',\n",
    " 'DayOfWeekRecode',\n",
    " 'Minute',\n",
    " 'Year',\n",
    " 'StreetCornerFlag',\n",
    " 'X',\n",
    " 'DistrictPARK',\n",
    " 'DistrictMISSION',\n",
    " 'Day',\n",
    " 'DistrictCENTRAL',\n",
    " 'DistrictINGLESIDE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submission(clf, Xtrain, ytrain, predictors, path='my_submissionXGBoostWithWeather.csv'):\n",
    "    '''This function will take in a trained model, a list of predictors, and an optional \n",
    "    filepath and create a submissision file for us.'''\n",
    "   \n",
    "    test_data = pd.read_csv('test.csv')\n",
    "    print \"test data loaded\"\n",
    "    \n",
    "    test_data, newColumns, streetColumns = recodeData(test_data)\n",
    "    print \"test data recoded\"\n",
    "    \n",
    "    testDataColumns = list(test_data.columns.values)\n",
    "    \n",
    "    existingPredictors = list(set(predictors) & set(testDataColumns))\n",
    "    \n",
    "    clf.fit(Xtrain[existingPredictors], ytrain, eval_metric = 'mlogloss')\n",
    "    print \"model fitted with all data\"\n",
    "    \n",
    "    #clf.fit(trainX[predictors], trainY)\n",
    "    predictions = clf.predict_proba(test_data[existingPredictors])\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        'Id': test_data.Id\n",
    "    })\n",
    "    \n",
    "    for i in range(predictions.shape[1]):\n",
    "        submission[Categories[i]] = predictions[:,i]\n",
    "    submission.to_csv(path, index=False)\n",
    "\n",
    "    print(\" -- Wrote submission to file {}.\".format(path))\n",
    "    return existingPredictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictorsUsed = make_submission(clf, X, y, columnsToUse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=10):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "clfOpt = xgb.XGBClassifier(objective='multi:softprob', nthread = 12, max_delta_step = 1)\n",
    "#(max_depth=7, n_estimators=100, objective='multi:softprob', max_delta_step = 1, learning_rate = .33)\n",
    "\n",
    "param_dist = {\"max_depth\": [5, 7, 9],\n",
    "              \"n_estimators\": [500, 1000],\n",
    "              \"learning_rate\": [0.033, .1]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clfOpt, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, scoring=\"log_loss\", verbose=3, cv=1)\n",
    "\n",
    "finalX = X[predictorsUsed].values.astype(np.float32)\n",
    "finaly = y.values.astype(int)\n",
    "\n",
    "print \"pre Random search finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"starting Randomized Search\"\n",
    "start = time()\n",
    "random_search.fit(finalX, finaly)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
